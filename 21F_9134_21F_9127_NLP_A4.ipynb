{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Muhammad Saad Habib , 21F-9079\n",
        "# Saad Nadeem , 21F-9104\n",
        "## 7D\n",
        "## Assignmnet # 4\n"
      ],
      "metadata": {
        "id": "ab4ZSCYCXtKg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8hDZvWz2lBn",
        "outputId": "5523aa60-e4e3-47ac-cacb-841ec6d730f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cpu)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.6)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=77e366f3d24ad399b5a8ab1c3a84b5cc8e7d1042b45f9719f310b14e0cbd5eab\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install pandas\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install rouge-score\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Designing the Transformer-Based LLM**"
      ],
      "metadata": {
        "id": "l0ihuIp4Ygw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Create a long enough 'pe' matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        # Compute the positional encodings once in log space.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
        "        pe = pe.unsqueeze(0)  # Shape: [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_len, d_model]\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "# Multi-Head Attention Mechanism\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.d_k = d_model // num_heads  # Dimension of each head\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Linear layers for Q, K, V\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output linear layer\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.scale = math.sqrt(self.d_k)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Linear projection and split into heads\n",
        "        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # [B, H, L, Dk]\n",
        "        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # [B, H, L, L]\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        context = torch.matmul(attn, V)  # [B, H, L, Dk]\n",
        "\n",
        "        # Concatenate heads\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)  # [B, L, D]\n",
        "\n",
        "        # Final linear layer\n",
        "        output = self.fc(context)\n",
        "        return output\n",
        "\n",
        "# Position-wise Feed-Forward Network\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply two linear transformations with ReLU activation in between\n",
        "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
        "\n",
        "# Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff=2048):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        # Self-attention with residual connection and layer normalization\n",
        "        src2 = self.norm1(src)\n",
        "        attn_output = self.self_attn(src2, src2, src2, src_mask)\n",
        "        src = src + self.dropout(attn_output)\n",
        "\n",
        "        # Feed-forward network with residual connection and layer normalization\n",
        "        src2 = self.norm2(src)\n",
        "        ffn_output = self.ffn(src2)\n",
        "        src = src + self.dropout(ffn_output)\n",
        "        return src\n",
        "\n",
        "# Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff=2048):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.norm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
        "        # Masked self-attention with residual connection and layer normalization\n",
        "        tgt2 = self.norm1(tgt)\n",
        "        attn_output = self.self_attn(tgt2, tgt2, tgt2, tgt_mask)\n",
        "        tgt = tgt + self.dropout(attn_output)\n",
        "\n",
        "        # Encoder-decoder attention with residual connection and layer normalization\n",
        "        tgt2 = self.norm2(tgt)\n",
        "        attn_output = self.enc_dec_attn(tgt2, memory, memory, memory_mask)\n",
        "        tgt = tgt + self.dropout(attn_output)\n",
        "\n",
        "        # Feed-forward network with residual connection and layer normalization\n",
        "        tgt2 = self.norm3(tgt)\n",
        "        ffn_output = self.ffn(tgt2)\n",
        "        tgt = tgt + self.dropout(ffn_output)\n",
        "        return tgt\n",
        "\n",
        "# Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff=2048, max_len=5000):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        # Embed and add positional encoding\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoding(src)\n",
        "        src = self.dropout(src)\n",
        "\n",
        "        # Pass through encoder layers\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        return src\n",
        "\n",
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff=2048, max_len=5000):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
        "        # Embed and add positional encoding\n",
        "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
        "        tgt = self.pos_encoding(tgt)\n",
        "        tgt = self.dropout(tgt)\n",
        "\n",
        "        # Pass through decoder layers\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, memory, tgt_mask, memory_mask)\n",
        "\n",
        "        # Final linear layer to generate predictions\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "# Full Transformer Model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512,\n",
        "                 num_layers=6, num_heads=8, d_ff=2048, max_len=5000):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, max_len)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, max_len)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        # Create a mask for padding tokens in the source sequences\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # Shape: [B, 1, 1, L_src]\n",
        "        return src_mask\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        # Create a mask for padding tokens and future tokens in the target sequences\n",
        "        tgt_pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)  # Shape: [B, 1, 1, L_tgt]\n",
        "        tgt_len = tgt.size(1)\n",
        "        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()  # Shape: [L_tgt, L_tgt]\n",
        "        tgt_mask = tgt_pad_mask & tgt_sub_mask  # Combine masks\n",
        "        return tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "\n",
        "        memory = self.encoder(src, src_mask)\n",
        "        output = self.decoder(tgt, memory, tgt_mask, src_mask)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "PBwjvyj-7X1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loading and Preprocessing**"
      ],
      "metadata": {
        "id": "o9_7g-cbZIg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/sample_data/samsum-test.csv')\n",
        "df = df[['dialogue', 'summary']]\n",
        "\n",
        "# Clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "df['dialogue'] = df['dialogue'].apply(clean_text)\n",
        "df['summary'] = df['summary'].apply(clean_text)\n",
        "\n",
        "# Tokenization\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize(text):\n",
        "    return [token.text.lower() for token in nlp.tokenizer(text)]\n",
        "\n",
        "# Vocabulary class\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.itos = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
        "        self.stoi = {v: k for k, v in self.itos.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def build_vocabulary(self, sentences):\n",
        "        frequencies = Counter()\n",
        "        idx = 4  # Starting index for new tokens\n",
        "\n",
        "        for sentence in sentences:\n",
        "            tokens = tokenize(sentence)\n",
        "            frequencies.update(tokens)\n",
        "\n",
        "        # Add tokens to vocabulary if they meet the frequency threshold\n",
        "        for word, freq in frequencies.items():\n",
        "            if freq >= self.freq_threshold:\n",
        "                self.stoi[word] = idx\n",
        "                self.itos[idx] = word\n",
        "                idx += 1\n",
        "\n",
        "def numericalize(text, vocab):\n",
        "    tokenized_text = tokenize(text)\n",
        "    return [\n",
        "        vocab.stoi.get(token, vocab.stoi['<UNK>'])\n",
        "        for token in tokenized_text\n",
        "    ]\n",
        "\n",
        "# Build vocabularies\n",
        "freq_threshold = 2\n",
        "src_vocab = Vocabulary(freq_threshold)\n",
        "tgt_vocab = Vocabulary(freq_threshold)\n",
        "\n",
        "src_vocab.build_vocabulary(df['dialogue'].tolist())\n",
        "tgt_vocab.build_vocabulary(df['summary'].tolist())\n",
        "\n",
        "print(f\"Source Vocabulary Size: {len(src_vocab)}\")\n",
        "print(f\"Target Vocabulary Size: {len(tgt_vocab)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfUncqWE7exY",
        "outputId": "80157911-3e2a-41ea-aeef-7e0f4f520599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Vocabulary Size: 3798\n",
            "Target Vocabulary Size: 1511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Preparation**"
      ],
      "metadata": {
        "id": "IYDFccb8ZTKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dataset class\n",
        "class SamSumDataset(Dataset):\n",
        "    def __init__(self, df, src_vocab, tgt_vocab, max_src_len=100, max_tgt_len=50):\n",
        "        self.df = df\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.max_src_len = max_src_len\n",
        "        self.max_tgt_len = max_tgt_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = self.df.iloc[idx]['dialogue']\n",
        "        tgt_text = self.df.iloc[idx]['summary']\n",
        "\n",
        "        # Numericalize\n",
        "        src_seq = numericalize(src_text, self.src_vocab)\n",
        "        tgt_seq = numericalize(tgt_text, self.tgt_vocab)\n",
        "\n",
        "        # Add <SOS> and <EOS> tokens to target sequence\n",
        "        tgt_seq = [self.tgt_vocab.stoi['<SOS>']] + tgt_seq + [self.tgt_vocab.stoi['<EOS>']]\n",
        "\n",
        "        # Truncate sequences to max lengths\n",
        "        src_seq = src_seq[:self.max_src_len]\n",
        "        tgt_seq = tgt_seq[:self.max_tgt_len]\n",
        "\n",
        "        return torch.tensor(src_seq), torch.tensor(tgt_seq)\n",
        "\n",
        "# Collate function\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "\n",
        "    # Pad sequences\n",
        "    src_batch = torch.nn.utils.rnn.pad_sequence(src_batch, padding_value=src_vocab.stoi['<PAD>'], batch_first=True)\n",
        "    tgt_batch = torch.nn.utils.rnn.pad_sequence(tgt_batch, padding_value=tgt_vocab.stoi['<PAD>'], batch_first=True)\n",
        "\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "# Split the DataFrame into training and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"Training samples: {len(train_df)}\")\n",
        "print(f\"Validation samples: {len(val_df)}\")\n",
        "\n",
        "# Create training and validation datasets\n",
        "train_dataset = SamSumDataset(train_df, src_vocab, tgt_vocab, max_src_len=100, max_tgt_len=50)\n",
        "val_dataset = SamSumDataset(val_df, src_vocab, tgt_vocab, max_src_len=100, max_tgt_len=50)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32  # Adjust based on your computational resources\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCyBp1T67nIy",
        "outputId": "0b8957c3-9492-4dd9-8001-914cea33dd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 737\n",
            "Validation samples: 82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training the Model**"
      ],
      "metadata": {
        "id": "kBFa1f0xZeEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize model\n",
        "src_vocab_size = len(src_vocab)\n",
        "tgt_vocab_size = len(tgt_vocab)\n",
        "model = Transformer(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    d_model=512,\n",
        "    num_layers=6,\n",
        "    num_heads=8,\n",
        "    d_ff=2048,\n",
        "    max_len=5000\n",
        ").to(device)\n",
        "\n",
        "# Initialize optimizer and scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Custom learning rate scheduler with warm-up\n",
        "class CustomSchedule(optim.lr_scheduler.LambdaLR):\n",
        "    def __init__(self, optimizer, d_model, warmup_steps=4000, last_epoch=-1):\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "        super(CustomSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
        "\n",
        "    def lr_lambda(self, step):\n",
        "        step = max(step, 1)\n",
        "        return (self.d_model ** -0.5) * min(step ** -0.5, step * (self.warmup_steps ** -1.5))\n",
        "\n",
        "scheduler = CustomSchedule(optimizer, d_model=512, warmup_steps=4000)\n",
        "\n",
        "# Initialize loss function\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=src_vocab.stoi['<PAD>'])\n",
        "\n",
        "# Initialize EarlyStopping\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How many epochs to wait after last time validation loss improved.\n",
        "            min_delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            return False\n",
        "\n",
        "        elif val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            return False\n",
        "\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "            return self.early_stop\n",
        "\n",
        "early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
        "\n",
        "# Initialize TensorBoard writer\n",
        "writer = SummaryWriter('runs/transformer_experiment')\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "best_val_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (src_batch, tgt_batch) in enumerate(train_loader):\n",
        "        src_batch = src_batch.to(device)\n",
        "        tgt_batch = tgt_batch.to(device)\n",
        "\n",
        "        # Prepare target sequences\n",
        "        tgt_input = tgt_batch[:, :-1]\n",
        "        tgt_output = tgt_batch[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src_batch, tgt_input)\n",
        "        output = output.view(-1, output.size(-1))\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, tgt_output)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src_batch, tgt_batch in val_loader:\n",
        "            src_batch = src_batch.to(device)\n",
        "            tgt_batch = tgt_batch.to(device)\n",
        "\n",
        "            tgt_input = tgt_batch[:, :-1]\n",
        "            tgt_output = tgt_batch[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = model(src_batch, tgt_input)\n",
        "            output = output.view(-1, output.size(-1))\n",
        "\n",
        "            loss = criterion(output, tgt_output)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    # Log losses to TensorBoard\n",
        "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
        "    writer.add_scalar('Loss/validation', avg_val_loss, epoch)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] completed in {elapsed_time:.2f}s')\n",
        "    print(f'Average Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "    # Early Stopping Check\n",
        "    if early_stopping(avg_val_loss):\n",
        "        print(\"Early stopping triggered. Stopping training.\")\n",
        "        break\n",
        "\n",
        "    # Save the model if validation loss has decreased\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), 'best_transformer_model.pth')\n",
        "        print(\"Model saved.\")\n",
        "\n",
        "# Close TensorBoard writer\n",
        "writer.close()\n",
        "\n",
        "print(\"Training completed.\")\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss', marker='o')\n",
        "plt.plot(val_losses, label='Validation Loss', marker='o')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "ze730uIF70SM",
        "outputId": "8f7ad0df-765d-4269-94bf-98690ffd93a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3] completed in 295.03s\n",
            "Average Training Loss: 57.0778, Validation Loss: 51.8451\n",
            "Model saved.\n",
            "Epoch [2/3] completed in 291.52s\n",
            "Average Training Loss: 56.7774, Validation Loss: 51.8450\n",
            "Model saved.\n",
            "Epoch [3/3] completed in 292.17s\n",
            "Average Training Loss: 56.8121, Validation Loss: 51.8447\n",
            "Model saved.\n",
            "Training completed.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation Metrics**"
      ],
      "metadata": {
        "id": "6HXB33sAZo5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer, scoring\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('best_transformer_model.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Inference function\n",
        "def generate_summary(model, src_sentence, src_vocab, tgt_vocab, max_len=50):\n",
        "    model.eval()\n",
        "    src_seq = numericalize(src_sentence, src_vocab)\n",
        "    src_seq = torch.tensor(src_seq).unsqueeze(0).to(device)\n",
        "    src_mask = model.make_src_mask(src_seq)\n",
        "\n",
        "    # Encode the source sequence\n",
        "    memory = model.encoder(src_seq, src_mask)\n",
        "\n",
        "    # Initialize the decoder input with <SOS>\n",
        "    ys = torch.ones(1, 1).fill_(tgt_vocab.stoi['<SOS>']).type(torch.long).to(device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        tgt_mask = model.make_tgt_mask(ys)\n",
        "        out = model.decoder(ys, memory, tgt_mask, src_mask)\n",
        "        out = out[:, -1, :]  # Get the last time step\n",
        "        prob = F.softmax(out, dim=-1)\n",
        "        next_word = torch.argmax(prob, dim=1).item()\n",
        "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src_seq).fill_(next_word)], dim=1)\n",
        "        if next_word == tgt_vocab.stoi['<EOS>']:\n",
        "            break\n",
        "\n",
        "    # Convert indices to words\n",
        "    predicted_tokens = [tgt_vocab.itos[idx] for idx in ys.squeeze().tolist()]\n",
        "    # Remove <SOS> and tokens after <EOS>\n",
        "    if '<EOS>' in predicted_tokens:\n",
        "        eos_index = predicted_tokens.index('<EOS>')\n",
        "        predicted_tokens = predicted_tokens[1:eos_index]\n",
        "    else:\n",
        "        predicted_tokens = predicted_tokens[1:]\n",
        "    return ' '.join(predicted_tokens)\n",
        "\n",
        "# Generate summaries and collect references\n",
        "references = []\n",
        "hypotheses = []\n",
        "\n",
        "for idx in tqdm(range(len(val_df))):\n",
        "    src_text = val_df.iloc[idx]['dialogue']\n",
        "    tgt_text = val_df.iloc[idx]['summary']\n",
        "\n",
        "    # Generate summary\n",
        "    generated_summary = generate_summary(model, src_text, src_vocab, tgt_vocab)\n",
        "\n",
        "    # Append to lists\n",
        "    references.append(tgt_text)\n",
        "    hypotheses.append(generated_summary)\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "aggregator = scoring.BootstrapAggregator()\n",
        "\n",
        "for ref, hyp in zip(references, hypotheses):\n",
        "    scores = scorer.score(ref, hyp)\n",
        "    aggregator.add_scores(scores)\n",
        "\n",
        "result = aggregator.aggregate()\n",
        "\n",
        "# Print ROUGE scores\n",
        "print(\"\\nCustom Model ROUGE Scores:\")\n",
        "print(\"ROUGE-1:\")\n",
        "print(f\"  Precision: {result['rouge1'].mid.precision:.4f}\")\n",
        "print(f\"  Recall:    {result['rouge1'].mid.recall:.4f}\")\n",
        "print(f\"  F1 Score:  {result['rouge1'].mid.fmeasure:.4f}\")\n",
        "\n",
        "print(\"ROUGE-2:\")\n",
        "print(f\"  Precision: {result['rouge2'].mid.precision:.4f}\")\n",
        "print(f\"  Recall:    {result['rouge2'].mid.recall:.4f}\")\n",
        "print(f\"  F1 Score:  {result['rouge2'].mid.fmeasure:.4f}\")\n",
        "\n",
        "print(\"ROUGE-L:\")\n",
        "print(f\"  Precision: {result['rougeL'].mid.precision:.4f}\")\n",
        "print(f\"  Recall:    {result['rougeL'].mid.recall:.4f}\")\n",
        "print(f\"  F1 Score:  {result['rougeL'].mid.fmeasure:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ievuLG6tJgQY",
        "outputId": "f6eba085-a721-43c6-fb87-610d2830c170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-60b9e53ecda6>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_transformer_model.pth'))\n",
            "100%|██████████| 82/82 [22:03<00:00, 16.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom Model ROUGE Scores:\n",
            "ROUGE-1:\n",
            "  Precision: 0.0155\n",
            "  Recall:    0.0379\n",
            "  F1 Score:  0.0205\n",
            "ROUGE-2:\n",
            "  Precision: 0.0000\n",
            "  Recall:    0.0000\n",
            "  F1 Score:  0.0000\n",
            "ROUGE-L:\n",
            "  Precision: 0.0140\n",
            "  Recall:    0.0345\n",
            "  F1 Score:  0.0186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sample Text and Summarization**"
      ],
      "metadata": {
        "id": "COXXf6oyZ8Bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a sample index from the validation set\n",
        "sample_index = 0  # Change this index to test different samples\n",
        "\n",
        "# Extract the sample dialogue and reference summary\n",
        "sample_dialogue = val_df.iloc[sample_index]['dialogue']\n",
        "reference_summary = val_df.iloc[sample_index]['summary']\n",
        "\n",
        "print(\"\\nSample Input Dialogue:\")\n",
        "print(sample_dialogue)\n",
        "\n",
        "print(\"\\nReference Summary:\")\n",
        "print(reference_summary)\n",
        "\n",
        "# Generate the predicted summary\n",
        "generated_summary = generate_summary(model, sample_dialogue, src_vocab, tgt_vocab)\n",
        "\n",
        "print(\"\\nGenerated Summary:\")\n",
        "print(generated_summary)\n",
        "\n",
        "# Compare and analyze the summaries\n",
        "print(\"\\nComparison:\")\n",
        "print(\"Reference Summary:\")\n",
        "print(reference_summary)\n",
        "\n",
        "print(\"\\nGenerated Summary:\")\n",
        "print(generated_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3drRY-rAPEW_",
        "outputId": "ac4d1a54-e72d-4372-dcde-ff1cdacb4c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Input Dialogue:\n",
            "Olafur: are we doing anything for New Year's Eve? Nathalie: I was thinking about something classy, like opera or sth like that Zoe: how much does it cost? Olafur: opera is not for me Nathalie: so what do you propose? Nathalie: it's 100$ Olafur: I was thinking about partying somewhere Nathalie: partying sounds fun, as long as it will be classy Zoe: <file_link> Zoe: Breakfast at Tiffany's party sounds classy Olafur: <file_link> Olafur: is it classy enough? Nathalie: :O Nathalie: this club is AMAZING Zoe: whoa Nathalie: we'll going to Soho then Olafur: we just need to hurry up and buy some tickets soon Zoe: sure\n",
            "\n",
            "Reference Summary:\n",
            "Nathalie, Olafur and Zoe are planning the New Year's Eve. Nathalie wants something classy. Olafur doesn't like opera. They want to go to the Breakfast at Tiffany's party in Soho.\n",
            "\n",
            "Generated Summary:\n",
            "left olivia year marcel leave think daniel marty phil supposed terry allergic book bottle delivery mom platform selling close graham special post but what elena likes pizza winnie join gym fight had ticket helen samuel jake soon patti outside course twice logan close graham special post but what elena 6th\n",
            "\n",
            "Comparison:\n",
            "Reference Summary:\n",
            "Nathalie, Olafur and Zoe are planning the New Year's Eve. Nathalie wants something classy. Olafur doesn't like opera. They want to go to the Breakfast at Tiffany's party in Soho.\n",
            "\n",
            "Generated Summary:\n",
            "left olivia year marcel leave think daniel marty phil supposed terry allergic book bottle delivery mom platform selling close graham special post but what elena likes pizza winnie join gym fight had ticket helen samuel jake soon patti outside course twice logan close graham special post but what elena 6th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_fUvui_jPJPl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}